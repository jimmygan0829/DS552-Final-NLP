{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code run on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ6MhJYYBCwu",
    "outputId": "66d80b46-1dd7-4ef8-d688-c93da41b2bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 26 00:18:59 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jjsbi1u3QFEM",
    "outputId": "91ef7ffc-052f-49cd-a254-875ff75a2b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 2.1MB 15.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.3MB 52.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 901kB 57.5MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w68CZpOwFoly",
    "outputId": "1f54f0bc-09b2-4b2c-a7f3-a90f42afaa74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Setup & Config\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "owqbRoU_tG7P"
   },
   "outputs": [],
   "source": [
    "dataText = pd.read_csv(\"./train140.csv\",encoding=\"latin-1\")\n",
    "testData = pd.read_csv(\"./test140.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCN7DTTxtLE9",
    "outputId": "f2d94f64-f3de-491d-c645-2115a3413498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re,nltk,string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def text_clean(x):\n",
    "    x = x.lower()\n",
    "    x = ' '.join([m for m in x.split(' ') if m not in stop_words])\n",
    "    x = x.encode('ascii','ignore').decode()## characters that ascii not recognized like emoji\n",
    "    x = re.sub(r'@\\S+', ' ', x)## @ handler\n",
    "    x = re.sub(r'#\\S+', ' ', x)## hashtag\n",
    "    x = re.sub(r'\\'\\w+', '', x)## ticks\n",
    "    x = re.sub(\"https*\\S+\", \" \", x)## http link\n",
    "    #x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)## numbers\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)## double space\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fErMYfLVtcN-"
   },
   "outputs": [],
   "source": [
    "def extractLabel(df):\n",
    "    output = df.label.copy()\n",
    "    output.replace({4:1},inplace=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dldYqJIvqgE",
    "outputId": "a4f577e7-f2d4-4a91-ba6e-0794a2f3dce0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    182\n",
       " 0    177\n",
       " Name: label, dtype: int64, 0    6005\n",
       " 1    5995\n",
       " Name: label, dtype: int64, 0    759\n",
       " 1    741\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean = dataText.copy()\n",
    "dataClean = dataClean.text.apply(text_clean)\n",
    "tvlabel = extractLabel(dataText)\n",
    "N = len(dataText)\n",
    "shuffled_indices = np.random.permutation(N)\n",
    "train_idx = shuffled_indices[:int(N*0.8)]\n",
    "\n",
    "valid_idx = shuffled_indices[int(N*0.8):]\n",
    "\n",
    "trainCon = dataClean.iloc[train_idx]\n",
    "validCon = dataClean.iloc[valid_idx]\n",
    "trainLB = tvlabel.iloc[train_idx]\n",
    "validLB = tvlabel.iloc[valid_idx]\n",
    "\n",
    "testClean = testData.copy()\n",
    "testClean = testClean[testClean[\"label\"]!=2]\n",
    "testlabel = extractLabel(testClean)\n",
    "testClean = testClean.text.apply(text_clean)\n",
    "\n",
    "testlabel.value_counts(), trainLB[:12000].value_counts(),validLB[:1500].value_counts(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "7af3a3dc01f34df4820588597cf30efd",
      "aeab8a2a69004d45a66b4fa3dbcdb367",
      "6882ec1eeb6a4abbab4789ce867e16ca",
      "a5670d55f668481b945cbecb43ce0375",
      "b8d49065aaa04eecb395a0a961acef3a",
      "23bf94776aca4066ab034fe06762757d",
      "3864fb7399db411798885306662a6cbc",
      "887611f49d384efcbbc91dfe6f1738c4",
      "37c7fbed84ff47a59842069ef78bb280",
      "006f52465c1a404c9810f2a5e32038fa",
      "ec141d7b12d241648e712a5496697655",
      "96676183d6b04c669154ff7621600330",
      "04711612c28d4339ac9808767e3b9726",
      "eb313ab6da814a899802256427ba5aac",
      "1a18a643fd0e4270b1d09c25add6d2b7",
      "bba89650c0d94a49977adfdf30ae7425",
      "0f82b01b338e49a4b777eeeaab08b46b",
      "050d5c580aec46d7a889c2d6bb7fb648",
      "0782843aa0344009a455b57393b6b432",
      "0a0934f8a5ad4c259bc8c6c38123636d",
      "68ccf73d8a33434994bfb493b6040d3a",
      "10bfe0ad402449769be33268664d0d13",
      "12db7514d6c54304b0a165d2bfa3eb1d",
      "c7d28a2558e9408f99861017c5a4fd5b"
     ]
    },
    "id": "H3AfJSZ8NNLF",
    "outputId": "07b00776-5df6-4160-944a-1c32adc0693c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af3a3dc01f34df4820588597cf30efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c7fbed84ff47a59842069ef78bb280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f82b01b338e49a4b777eeeaab08b46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(content,labels, tokenizer, max_len, batch_size):\n",
    "  ds = GPReviewDataset(\n",
    "    reviews=content.to_numpy(),\n",
    "    targets=labels.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uatJybrqnV_K",
    "outputId": "5b2f8e24-20f3-44d4-a1a7-c3ee69f345c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    182\n",
       " 0    177\n",
       " Name: label, dtype: int64, 0    18037\n",
       " 1    17963\n",
       " Name: label, dtype: int64, 0    2063\n",
       " 1    1937\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlabel.value_counts(), trainLB[:36000].value_counts(),validLB[:4000].value_counts(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vODDxMKsPHqI",
    "outputId": "dac8fb21-ad9c-4540-8d39-cdc946469a46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(trainCon[:36000],trainLB[:36000], tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(validCon[:4000],validLB[:4000], tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(testClean,testlabel, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y93ldSN47FeT",
    "outputId": "ae45133a-6fd5-43fd-c1ca-8bd717623ace"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdU4YVqb7N8M",
    "outputId": "f765a2c0-6400-4298-d634-52aec8332fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "0e56c81baa584c6c87f55eebddf8d17a",
      "c811734fbe9c4d20b3cf257b8401d737",
      "617da169fbf847c2873d57fc59b2005b",
      "d76b9515eaf24aaca22174cbff273a69",
      "ab8dec8730ad41cca3a17f7e86e26a98",
      "7eaa2e016b3240d0bbc7ed28986d52f7",
      "f4c69389c5aa4990acf57ae573fba1f6",
      "12cfd252ccb6496982102348d6d466b7",
      "62a4c85b4b5d47ce9b119cf9c45a40e3",
      "9adcc3d9c9434b55ade671b7c53650a4",
      "5b4d07cbb4e24bfd9193574e6cd2b221",
      "5e10f563e1b74839ac59f432f4529506",
      "515f913d9c6f499fb81d91d827c01825",
      "4cc3e6be5e87484fa8dcab21deb0e910",
      "1216683bfcbb41609192abd10672589d",
      "3edbade71f7f48e2ac9be3fb59f83658"
     ]
    },
    "id": "0P41FayISNRI",
    "outputId": "e2cc9546-3a17-46b6-acb6-4e37e8d9e368"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e56c81baa584c6c87f55eebddf8d17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a4c85b4b5d47ce9b119cf9c45a40e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    returned = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    pooled_output = returned['pooler_output']\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [],
   "source": [
    "model = SentimentClassifier(2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mz7p__CqdaMO",
    "outputId": "34c93709-a850-4f8a-d986-1e2caa29c36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zhHoFNsxufs",
    "outputId": "c7898c98-f61d-4865-ec6a-0c65d2dcd357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5043598844342762 accuracy 0.758888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.46009408193826673 accuracy 0.77775\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.36248891251368653 accuracy 0.8489722222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.4964302020967007 accuracy 0.7815\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.25648230311067566 accuracy 0.9093611111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6319201344251633 accuracy 0.779\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.19199858007378257 accuracy 0.9441111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.9341797448955477 accuracy 0.77325\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.15139001561287377 accuracy 0.9610833333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.07747156969551 accuracy 0.775\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.12345588882559808 accuracy 0.9693333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.0934774557426572 accuracy 0.77275\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.09207354954659887 accuracy 0.9774166666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.3089561926308089 accuracy 0.769\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.06985227220041108 accuracy 0.9832222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.4854302361442242 accuracy 0.77175\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.05715069164878999 accuracy 0.9868611111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.5506404209351168 accuracy 0.77175\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.04597658237205016 accuracy 0.9889166666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.6071462103627856 accuracy 0.7735\n",
      "\n",
      "CPU times: user 45min 7s, sys: 21min 55s, total: 1h 7min 2s\n",
      "Wall time: 1h 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    36000#len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    4000 #len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "-FWG7kBm372V",
    "outputId": "9ae04495-6c99-4e28-842a-ba041fd432bc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 506,
       "width": 732
      },
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jS3gJ_qBEljD",
    "outputId": "47b6d69f-5d56-4c56-a6a9-0a520b6bdf04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8217270194986073"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(testClean)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"review_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHdPZr60-0c_",
    "outputId": "c0b9da02-872f-4f07-9b94-78d525c6e307"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSnkWBvbK6v6",
    "outputId": "ba8d75ae-8a0e-41ea-fdb5-a7c4ec93e5ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[144,  33],\n",
       "       [ 31, 151]])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "pred_np = y_pred.numpy()\n",
    "testlb_np = testlabel.to_numpy()\n",
    "sklearn.metrics.confusion_matrix(testlb_np,pred_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaN4RnqMnxYw"
   },
   "source": [
    "We have to use the tokenizer to encode the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"I love going to shopping.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zA5Or4D2sLc9",
    "outputId": "c979115f-79b0-4f5d-cb50-f6933c2b1542"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "  review_text,\n",
    "  max_length=MAX_LEN,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qr_t3rUksumr",
    "outputId": "294ef3d6-0019-42eb-a6f5-43e9f5212246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: I love going to shopping.\n",
      "Sentiment  : positive\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "class_names = ['negative', 'positive']\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "    Venelin Valkov, Sentiment Analysis with BERT and Transformers by Hugging Face using PyTorch and Python. 4.20.2020. https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "006f52465c1a404c9810f2a5e32038fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04711612c28d4339ac9808767e3b9726": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "050d5c580aec46d7a889c2d6bb7fb648": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0782843aa0344009a455b57393b6b432": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10bfe0ad402449769be33268664d0d13",
      "max": 435797,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68ccf73d8a33434994bfb493b6040d3a",
      "value": 435797
     }
    },
    "0a0934f8a5ad4c259bc8c6c38123636d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7d28a2558e9408f99861017c5a4fd5b",
      "placeholder": "​",
      "style": "IPY_MODEL_12db7514d6c54304b0a165d2bfa3eb1d",
      "value": " 436k/436k [00:00&lt;00:00, 953kB/s]"
     }
    },
    "0e56c81baa584c6c87f55eebddf8d17a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_617da169fbf847c2873d57fc59b2005b",
       "IPY_MODEL_d76b9515eaf24aaca22174cbff273a69"
      ],
      "layout": "IPY_MODEL_c811734fbe9c4d20b3cf257b8401d737"
     }
    },
    "0f82b01b338e49a4b777eeeaab08b46b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0782843aa0344009a455b57393b6b432",
       "IPY_MODEL_0a0934f8a5ad4c259bc8c6c38123636d"
      ],
      "layout": "IPY_MODEL_050d5c580aec46d7a889c2d6bb7fb648"
     }
    },
    "10bfe0ad402449769be33268664d0d13": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1216683bfcbb41609192abd10672589d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12cfd252ccb6496982102348d6d466b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12db7514d6c54304b0a165d2bfa3eb1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a18a643fd0e4270b1d09c25add6d2b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23bf94776aca4066ab034fe06762757d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37c7fbed84ff47a59842069ef78bb280": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec141d7b12d241648e712a5496697655",
       "IPY_MODEL_96676183d6b04c669154ff7621600330"
      ],
      "layout": "IPY_MODEL_006f52465c1a404c9810f2a5e32038fa"
     }
    },
    "3864fb7399db411798885306662a6cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3edbade71f7f48e2ac9be3fb59f83658": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cc3e6be5e87484fa8dcab21deb0e910": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "515f913d9c6f499fb81d91d827c01825": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5b4d07cbb4e24bfd9193574e6cd2b221": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc3e6be5e87484fa8dcab21deb0e910",
      "max": 435779157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_515f913d9c6f499fb81d91d827c01825",
      "value": 435779157
     }
    },
    "5e10f563e1b74839ac59f432f4529506": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3edbade71f7f48e2ac9be3fb59f83658",
      "placeholder": "​",
      "style": "IPY_MODEL_1216683bfcbb41609192abd10672589d",
      "value": " 436M/436M [00:07&lt;00:00, 54.7MB/s]"
     }
    },
    "617da169fbf847c2873d57fc59b2005b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7eaa2e016b3240d0bbc7ed28986d52f7",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab8dec8730ad41cca3a17f7e86e26a98",
      "value": 570
     }
    },
    "62a4c85b4b5d47ce9b119cf9c45a40e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b4d07cbb4e24bfd9193574e6cd2b221",
       "IPY_MODEL_5e10f563e1b74839ac59f432f4529506"
      ],
      "layout": "IPY_MODEL_9adcc3d9c9434b55ade671b7c53650a4"
     }
    },
    "6882ec1eeb6a4abbab4789ce867e16ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23bf94776aca4066ab034fe06762757d",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8d49065aaa04eecb395a0a961acef3a",
      "value": 213450
     }
    },
    "68ccf73d8a33434994bfb493b6040d3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7af3a3dc01f34df4820588597cf30efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6882ec1eeb6a4abbab4789ce867e16ca",
       "IPY_MODEL_a5670d55f668481b945cbecb43ce0375"
      ],
      "layout": "IPY_MODEL_aeab8a2a69004d45a66b4fa3dbcdb367"
     }
    },
    "7eaa2e016b3240d0bbc7ed28986d52f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "887611f49d384efcbbc91dfe6f1738c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96676183d6b04c669154ff7621600330": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bba89650c0d94a49977adfdf30ae7425",
      "placeholder": "​",
      "style": "IPY_MODEL_1a18a643fd0e4270b1d09c25add6d2b7",
      "value": " 29.0/29.0 [00:00&lt;00:00, 70.5B/s]"
     }
    },
    "9adcc3d9c9434b55ade671b7c53650a4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5670d55f668481b945cbecb43ce0375": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_887611f49d384efcbbc91dfe6f1738c4",
      "placeholder": "​",
      "style": "IPY_MODEL_3864fb7399db411798885306662a6cbc",
      "value": " 213k/213k [00:01&lt;00:00, 116kB/s]"
     }
    },
    "ab8dec8730ad41cca3a17f7e86e26a98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aeab8a2a69004d45a66b4fa3dbcdb367": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8d49065aaa04eecb395a0a961acef3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bba89650c0d94a49977adfdf30ae7425": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7d28a2558e9408f99861017c5a4fd5b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c811734fbe9c4d20b3cf257b8401d737": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d76b9515eaf24aaca22174cbff273a69": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12cfd252ccb6496982102348d6d466b7",
      "placeholder": "​",
      "style": "IPY_MODEL_f4c69389c5aa4990acf57ae573fba1f6",
      "value": " 570/570 [00:00&lt;00:00, 1.87kB/s]"
     }
    },
    "eb313ab6da814a899802256427ba5aac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec141d7b12d241648e712a5496697655": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb313ab6da814a899802256427ba5aac",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04711612c28d4339ac9808767e3b9726",
      "value": 29
     }
    },
    "f4c69389c5aa4990acf57ae573fba1f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
